# AIME 2025 benchmark configuration
# Uses 2x A100 tensor parallel for longer context
env_id: aime2025

infra:
  gpu: "A100-80GB:4"

vllm:
  max_model_len: 32768
  tensor_parallel_size: 4
  data_parallel_size: 1

sampling:
  temperature: 0.8
  top_p: 0.95
  max_tokens: 28672

eval:
  rollouts: 32
  num_examples: 30
  max_concurrent: 48

chat_template: math1
